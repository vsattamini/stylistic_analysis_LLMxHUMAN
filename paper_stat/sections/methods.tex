% Methods

\subsection{Mineração de Texto e Pré-processamento}

A mineração de texto consiste em extrair informações úteis de dados textuais não estruturados através de técnicas estatísticas e computacionais~\cite{feldman2007}. O processo envolve etapas de coleta, pré-processamento (limpeza, tokenização, normalização), extração de características numéricas e aplicação de métodos analíticos. Neste trabalho, aplicamos mineração de texto para transformar documentos em vetores de variáveis quantitativas que capturam propriedades estatísticas do estilo de escrita, permitindo análise estatística inferencial e construção de modelos de classificação.

\subsection{Conjunto de Dados}

Utilizou-se um conjunto de dados textuais balanceado em português do Brasil contendo 100.000 amostras (50.000 autorais, 50.000 de LLMs), extraídas por amostragem estratificada de um conjunto maior com 2.331.317 documentos originais provenientes de 5 fontes distintas. As fontes de texto autoral incluem: (i) BrWaC (Brazilian Web as Corpus)~\cite{brwac}, um grande conjunto web de textos brasileiros; e (ii) BoolQ~\cite{boolq}, contendo passagens de contexto para perguntas booleanas. As fontes de texto gerado por LLM incluem: (i) ShareGPT-Portuguese~\cite{sharegpt_portuguese}, conversas em português extraídas da plataforma ShareGPT; (ii) resenhas do IMDB traduzidas para português por modelos de tradução automática (classificadas como texto LLM); e (iii) o dataset Canarim~\cite{canarim}, contendo saídas geradas por LLMs.

\subsubsection{Método de Amostragem Estratificada}

A amostragem foi realizada através de \textbf{amostragem aleatória estratificada proporcional} com estratificação por fonte de origem dos textos. Este método garante representatividade de cada fonte na amostra final.

\textbf{Procedimento}:

\begin{enumerate}
    \item \textbf{Definição de estratos}: A população foi dividida em $L = 5$ estratos correspondentes às fontes:
    \begin{itemize}
        \item Estrato 1: BrWaC (textos web humanos)
        \item Estrato 2: BoolQ traduzido (textos humanos)
        \item Estrato 3: ShareGPT-Portuguese (LLM conversacional)
        \item Estrato 4: IMDB traduzido (LLM)
        \item Estrato 5: Canarim-Instruct (LLM instrucional)
    \end{itemize}

    \item \textbf{Cálculo dos tamanhos amostrais por estrato}: Para amostragem proporcional com tamanho total $n = 100.000$:
    $$n_h = n \times \frac{N_h}{N}$$
    onde $N_h$ é o tamanho populacional do estrato $h$ e $N = \sum_{h=1}^{L} N_h = 2.331.317$ é o tamanho populacional total.

    \item \textbf{Seleção aleatória simples dentro de cada estrato}: Utilizamos \texttt{numpy.random.choice} com semente fixa (42) para reprodutibilidade, sem reposição.

    \item \textbf{Combinação das amostras estratificadas}: A amostra final é a união $\bigcup_{h=1}^{L} s_h$ onde $s_h$ é a amostra do estrato $h$.
\end{enumerate}

\textbf{Vantagens da estratificação}:
\begin{itemize}
    \item \textbf{Representatividade}: Garante presença de todas as fontes proporcionalmente ao tamanho populacional
    \item \textbf{Redução de variância}: A variância da estimativa é menor que na amostragem aleatória simples quando há heterogeneidade entre estratos
    \item \textbf{Estimativas por estrato}: Permite análises separadas por fonte quando necessário
\end{itemize}

\textbf{Justificativa estatística}: A estratificação por fonte é apropriada pois diferentes fontes podem ter características textuais distintas (e.g., BrWaC contém textos web informais; Canarim contém instruções formais). A amostragem proporcional mantém a distribuição populacional original, evitando viés de seleção.

Os textos foram previamente filtrados por comprimento mínimo de 100 caracteres e máximo de 10.000 caracteres, sendo textos muito longos segmentados em fragmentos de até 10.000 caracteres sem sobreposição. A segmentação priorizou quebras naturais de texto (pontos finais, parágrafos e espaços). O balanceamento foi obtido por subamostragem da classe majoritária e sobreamostragem da classe minoritária, resultando em proporções exatamente iguais (50\%/50\%). A amostra de 100.000 documentos foi selecionada aleatoriamente com semente fixa (\texttt{seed=42}) para reprodutibilidade.

Para prevenir vazamento de dados, verificamos que os textos não apresentam agrupamentos estruturais por autor, tópico ou sessão de geração. A validação cruzada estratificada mantém o balanço de classes entre as partições, garantindo amostras independentes em conjuntos de treino e teste. Esta abordagem evita viés de avaliação documentado em estudos anteriores~\cite{kohavi1995}.

\subsection{Extração de Características Estilométricas}
\label{sec:features}

Foram extraídas 10 características estilométricas de cada documento, todas representando variáveis contínuas. A escolha dessas características baseia-se em estudos anteriores que demonstraram sua eficácia na análise de autoria \cite{stamatatos2009,stylometric_llm_detection}.

\subsubsection{Variáveis em Escala de Razão}

As nove características a seguir são mensuradas em \textbf{escala de razão}, possuindo zero absoluto e permitindo interpretação de razões:

\begin{enumerate}
    \item \textbf{Comprimento médio de frase} (\texttt{sent\_mean}): Média aritmética do número de palavras por frase. Unidade: palavras/frase. Zero representa ausência de palavras.

    \item \textbf{Desvio padrão do comprimento de frase} (\texttt{sent\_std}): Medida de dispersão absoluta do comprimento de frases. Unidade: palavras. Quantifica a variabilidade no comprimento das frases.

    \item \textbf{Coeficiente de variação do comprimento de frase} (\texttt{sent\_cv}): Razão entre desvio padrão e média ($CV = \sigma/\mu$). Estatística adimensional que normaliza a variabilidade pela tendência central, permitindo comparação entre distribuições com escalas distintas \cite{madsen2005}.

    \item \textbf{Riqueza lexical - C de Herdan} (\texttt{herdan\_c}): Medida de diversidade vocabular calculada como $C = \log(V) / \log(N)$, onde $V$ é o número de tipos (palavras distintas) e $N$ é o número de tokens (total de palavras) \cite{herdan1960}. Varia entre 0 e 1, onde valores próximos a 1 indicam maior diversidade lexical.

    \item \textbf{Proporção de pontuação} (\texttt{punct\_ratio}): Razão entre número de sinais de pontuação e total de caracteres. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de dígitos} (\texttt{digit\_ratio}): Razão entre dígitos numéricos e total de caracteres. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de letras maiúsculas} (\texttt{upper\_ratio}): Razão entre letras maiúsculas e total de letras. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de palavras funcionais} (\texttt{func\_ratio}): Razão entre palavras funcionais (artigos, preposições, conjunções, pronomes) e total de palavras \cite{stamatatos2009}. Adimensional, varia entre 0 e 1. Palavras funcionais são frequentes e pouco conscientes, revelando estilo autoral.

    \item \textbf{Comprimento médio de palavra} (\texttt{word\_len\_mean}): Média do número de caracteres por palavra. Unidade: caracteres/palavra.
\end{enumerate}

\subsubsection{Variável em Escala de Intervalo}

\begin{enumerate}
    \setcounter{enumi}{9}
    \item \textbf{Variabilidade da distribuição de caracteres} (\texttt{char\_entropy}): Medida de dispersão na distribuição de frequências de caracteres, calculada pela fórmula de Shannon $H = -\sum_{c} p(c) \log_2 p(c)$ \cite{shannon1948}, onde $p(c)$ é a probabilidade de ocorrência do caractere $c$.

    Esta medida quantifica a variabilidade: alta entropia indica distribuição mais uniforme (maior dispersão); baixa entropia indica concentração (menor dispersão).

    \textbf{Justificativa estatística}: Embora originalmente uma medida da teoria da informação, a entropia funciona como \textbf{medida de dispersão análoga ao desvio padrão}, mas aplicada a distribuições de frequência categórica. A entropia é mensurada em \textbf{escala de intervalo} porque:
    \begin{itemize}
        \item Diferenças entre valores são interpretáveis (aumento de 1 bit representa dobrar a incerteza)
        \item Não possui zero absoluto natural (zero ocorre apenas com um único caractere)
        \item Razões entre valores não são estatisticamente interpretáveis
    \end{itemize}
\end{enumerate}

\subsubsection{Justificativa da Escolha das Características}

Todas as características foram selecionadas por três critérios:

\begin{enumerate}
    \item \textbf{Objetividade}: Mensuração automática e determinística, sem julgamento subjetivo.
    \item \textbf{Robustez}: Insensibilidade a pequenas variações no texto ou erros de tokenização.
    \item \textbf{Fundamentação teórica}: Suporte empírico na literatura de estilometria para distinção de autoria.
\end{enumerate}

A combinação de variáveis em escala de razão e intervalo permite aplicação de métodos estatísticos diversos. As variáveis de razão satisfazem requisitos para testes paramétricos quando distribuídas normalmente. A variável de entropia, sendo contínua em escala de intervalo, pode ser incluída em análises multivariadas que não assumem proporcionalidade (como PCA e regressão logística).

\subsection{Testes Estatísticos Não Paramétricos}
\label{sec:tests}

A escolha de métodos não paramétricos foi determinada pelas características das distribuições observadas nos dados, seguindo os critérios estabelecidos por Siegel e Castellan~\cite{siegel1988} e Hollander, Wolfe e Chicken~\cite{hollander2013}.

\subsubsection{Justificativa para Métodos Não Paramétricos}

Após análise exploratória inicial, identificamos três violações aos pressupostos de testes paramétricos:

\begin{enumerate}
    \item \textbf{Não normalidade}: Testes de Shapiro-Wilk ($\alpha = 0.05$) rejeitaram a hipótese de normalidade para 8 das 10 variáveis em ambos os grupos (humano e LLM).

    \item \textbf{Heterocedasticidade}: Teste de Levene indicou variâncias significativamente diferentes entre grupos para 6 variáveis ($p < 0.01$).

    \item \textbf{Presença de valores atípicos}: Boxplots revelaram outliers em 7 das 10 variáveis, com alguns valores extremos além de 3 desvios padrão da média.
\end{enumerate}

Dadas essas violações, métodos não paramétricos são mais apropriados pois:
\begin{itemize}
    \item Não assumem forma específica de distribuição
    \item São robustos a outliers (baseiam-se em postos, não valores brutos)
    \item Mantêm poder estatístico adequado com distribuições não normais
\end{itemize}

\subsubsection{Teste de Mann-Whitney U}

Para comparar as distribuições de cada variável entre textos humanos e LLM, utilizamos o teste de Mann-Whitney U \cite{mann1947}, também conhecido como teste de Wilcoxon para amostras independentes.

\textbf{Hipóteses}:
\begin{itemize}
    \item $H_0$: As distribuições das duas populações são idênticas
    \item $H_1$: As distribuições diferem em localização (mediana)
\end{itemize}

\textbf{Estatística do teste}:
$$U = n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1$$

onde $n_1$ e $n_2$ são os tamanhos amostrais, e $R_1$ é a soma dos postos do grupo 1.

\textbf{Interpretação}: Valores pequenos de $U$ (ou valores-$p$ menores que $\alpha$) indicam evidência contra $H_0$, sugerindo que as distribuições diferem sistematicamente.

\subsubsection{Tamanho de Efeito: Delta de Cliff}

O valor-$p$ indica apenas se há diferença estatisticamente detectável, não sua magnitude prática. Portanto, calculamos o Delta de Cliff ($\delta$) \cite{cliff1993} como medida de tamanho de efeito:

$$\delta = \frac{\#(x_i > y_j) - \#(x_i < y_j)}{n_1 \times n_2}$$

onde $x_i$ são observações do grupo 1 e $y_j$ do grupo 2.

\textbf{Interpretação} \cite{romano2006}:
\begin{itemize}
    \item $|\delta| < 0.147$: Efeito negligenciável
    \item $0.147 \leq |\delta| < 0.330$: Efeito pequeno
    \item $0.330 \leq |\delta| < 0.474$: Efeito médio
    \item $|\delta| \geq 0.474$: Efeito grande
\end{itemize}

O Delta de Cliff varia entre $-1$ e $+1$. Valores positivos indicam que o grupo 1 tende a ter valores maiores; negativos indicam o contrário.

\subsubsection{Correção para Comparações Múltiplas}

Realizamos 10 testes simultâneos (um por variável), inflando a taxa de erro Tipo I. Para controlar a \textbf{Taxa de Falsa Descoberta} (FDR - \textit{False Discovery Rate}), aplicamos o procedimento de Benjamini-Hochberg \cite{benjamini1995}:

\begin{enumerate}
    \item Ordenar os valores-$p$: $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(10)}$
    \item Para $\alpha = 0.05$, encontrar o maior $i$ tal que:
    $$p_{(i)} \leq \frac{i}{10} \times 0.05$$
    \item Rejeitar $H_0$ para todos os testes $1, 2, \ldots, i$
\end{enumerate}

Este procedimento controla a proporção esperada de falsos positivos entre as hipóteses rejeitadas, sendo menos conservador que a correção de Bonferroni.

\subsubsection{Implementação}

Todos os testes foram implementados em Python utilizando \texttt{scipy.stats} (versão 1.11.0). Valores-$p$ foram calculados com aproximação normal para amostras grandes ($n > 20$). O Delta de Cliff foi calculado com a biblioteca \texttt{cliffs\_delta} (versão 1.0.0).

\subsection{Análise de Componentes Principais (PCA)}

Para visualizar a estrutura multivariada dos dados, aplicamos análise de componentes principais~\cite{jolliffe2002} às 10 características estilométricas. As variáveis foram previamente padronizadas (média zero, desvio padrão unitário) usando \texttt{StandardScaler} do scikit-learn~\cite{scikit-learn}. Retemos os dois primeiros componentes principais (PC1 e PC2) para visualização bidimensional. Reportamos a proporção de variância explicada por cada componente e os pesos (cargas fatoriais) de cada característica original sobre os componentes.

\subsection{Modelos de Classificação}

Avaliamos três modelos para classificação binária:

\begin{enumerate}
    \item \textbf{Análise Discriminante Linear (LDA):} um classificador generativo que assume distribuições Gaussianas multivariadas para cada classe com matrizes de covariância iguais, buscando a direção de projeção $w = S_W^{-1}(\mu_1 - \mu_2)$ que maximiza a separação entre classes~\cite{fisher1936, mclachlan2004}.

    \item \textbf{Regressão Logística:} um modelo discriminativo que estima diretamente a probabilidade posterior através da função logística $P(Y=1|X) = 1 / (1 + \exp(-(\beta_0 + \sum \beta_i x_i)))$, sem assumir normalidade das características~\cite{hosmer2013}.

    \item \textbf{Classificador Fuzzy:} um sistema baseado em regras com funções de pertinência triangulares orientadas por dados (definidas por quantis 33\%, 50\%, 66\%), agregação por média aritmética e inferência tipo Takagi-Sugeno ordem-zero. Detalhes completos em trabalho complementar sobre classificação fuzzy.
\end{enumerate}

Os modelos LDA e Regressão Logística foram treinados sobre as 10 características padronizadas (média zero, desvio padrão unitário). Para a regressão logística, utilizamos \texttt{max\_iter=1000} e sem regularização. O classificador fuzzy opera diretamente sobre as características não-padronizadas.

\subsection{Validação Cruzada e Métricas de Desempenho}

Empregamos validação cruzada estratificada com 5 partições (\texttt{StratifiedKFold}, \texttt{random\_state=42})~\cite{kohavi1995} para avaliar o desempenho dos classificadores. A estratificação garante que cada partição mantenha a proporção 50/50 de classes. Cada partição utiliza 80\% dos dados para treino (4 partições) e 20\% para teste (1 partição).

A métrica primária de avaliação é a \textbf{área sob a curva ROC (AUC)}~\cite{fawcett2006}, que resume a capacidade do modelo de discriminar entre as classes em todos os limiares de decisão. A curva ROC representa graficamente a taxa de verdadeiros positivos (sensibilidade) versus a taxa de falsos positivos (1 - especificidade) para diferentes limiares de decisão. O AUC possui interpretação probabilística: a probabilidade de que um texto LLM aleatório receba pontuação maior que um texto autoral aleatório.

Reportamos a média e o desvio padrão de AUC ao longo das 5 partições. Todas as análises foram implementadas em Python 3 utilizando as bibliotecas pandas~\cite{pandas}, NumPy~\cite{numpy}, scikit-learn~\cite{scikit-learn} e SciPy~\cite{scipy} para testes estatísticos.
