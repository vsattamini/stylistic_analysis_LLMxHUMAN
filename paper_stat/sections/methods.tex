% Methods

\subsection{Mineração de Texto e Pré-processamento}

A mineração de texto consiste em extrair informações úteis de dados textuais não estruturados através de técnicas estatísticas e computacionais~\cite{feldman2007}. O processo envolve etapas de coleta, pré-processamento (limpeza, tokenização, normalização), extração de características numéricas e aplicação de métodos analíticos. Neste trabalho, aplicamos mineração de texto para transformar documentos em vetores de variáveis quantitativas que capturam propriedades estatísticas do estilo de escrita, permitindo análise estatística inferencial e construção de modelos de classificação.

\subsection{Conjunto de Dados}

Utilizou-se um conjunto de dados textuais balanceado em português do Brasil contendo 100.000 amostras (50.000 autorais, 50.000 de LLMs), extraídas por amostragem estratificada de um conjunto maior com 2.331.317 documentos originais provenientes de 5 fontes distintas. As fontes de texto autoral incluem: (i) BrWaC (Brazilian Web as Corpus)~\cite{brwac}, um grande conjunto web de textos brasileiros; e (ii) BoolQ~\cite{boolq}, contendo passagens de contexto para perguntas booleanas. As fontes de texto gerado por LLM incluem: (i) ShareGPT-Portuguese~\cite{sharegpt_portuguese}, conversas em português extraídas da plataforma ShareGPT; (ii) resenhas do IMDB traduzidas para português por modelos de tradução automática (classificadas como texto LLM); e (iii) o dataset Canarim~\cite{canarim}, contendo saídas geradas por LLMs.

Os textos foram previamente filtrados por comprimento mínimo de 100 caracteres e máximo de 10.000 caracteres, sendo textos muito longos segmentados em fragmentos de até 10.000 caracteres sem sobreposição. A segmentação priorizou quebras naturais de texto (pontos finais, parágrafos e espaços). O balanceamento foi obtido por subamostragem da classe majoritária e sobreamostragem da classe minoritária, resultando em proporções exatamente iguais (50\%/50\%). A amostra de 100.000 documentos foi selecionada aleatoriamente com semente fixa (\texttt{seed=42}) para reprodutibilidade.

Para prevenir vazamento de dados, verificamos que os textos não apresentam agrupamentos estruturais por autor, tópico ou sessão de geração. A validação cruzada estratificada mantém o balanço de classes entre as partições, garantindo amostras independentes em conjuntos de treino e teste. Esta abordagem evita viés de avaliação documentado em estudos anteriores~\cite{kohavi1995}.

\subsection{Extração de Características Estilométricas}
\label{sec:features}

Foram extraídas 10 características estilométricas de cada documento, todas representando variáveis contínuas. A escolha dessas características baseia-se em estudos anteriores que demonstraram sua eficácia na análise de autoria \cite{stamatatos2009,stylometric_llm_detection}.

\subsubsection{Variáveis em Escala de Razão}

As nove características a seguir são mensuradas em \textbf{escala de razão}, possuindo zero absoluto e permitindo interpretação de razões:

\begin{enumerate}
    \item \textbf{Comprimento médio de frase} (\texttt{sent\_mean}): Média aritmética do número de palavras por frase. Unidade: palavras/frase. Zero representa ausência de palavras.

    \item \textbf{Desvio padrão do comprimento de frase} (\texttt{sent\_std}): Medida de dispersão absoluta do comprimento de frases. Unidade: palavras. Quantifica a variabilidade no comprimento das frases.

    \item \textbf{Coeficiente de variação do comprimento de frase} (\texttt{sent\_cv}): Razão entre desvio padrão e média ($CV = \sigma/\mu$). Estatística adimensional que normaliza a variabilidade pela tendência central, permitindo comparação entre distribuições com escalas distintas \cite{madsen2005}.

    \item \textbf{Riqueza lexical - C de Herdan} (\texttt{herdan\_c}): Medida de diversidade vocabular calculada como $C = \log(V) / \log(N)$, onde $V$ é o número de tipos (palavras distintas) e $N$ é o número de tokens (total de palavras) \cite{herdan1960}. Varia entre 0 e 1, onde valores próximos a 1 indicam maior diversidade lexical.

    \item \textbf{Proporção de pontuação} (\texttt{punct\_ratio}): Razão entre número de sinais de pontuação e total de caracteres. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de dígitos} (\texttt{digit\_ratio}): Razão entre dígitos numéricos e total de caracteres. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de letras maiúsculas} (\texttt{upper\_ratio}): Razão entre letras maiúsculas e total de letras. Adimensional, varia entre 0 e 1.

    \item \textbf{Proporção de palavras funcionais} (\texttt{func\_ratio}): Razão entre palavras funcionais (artigos, preposições, conjunções, pronomes) e total de palavras \cite{stamatatos2009}. Adimensional, varia entre 0 e 1. Palavras funcionais são frequentes e pouco conscientes, revelando estilo autoral.

    \item \textbf{Comprimento médio de palavra} (\texttt{word\_len\_mean}): Média do número de caracteres por palavra. Unidade: caracteres/palavra.
\end{enumerate}

\subsubsection{Variável em Escala de Intervalo}

\begin{enumerate}
    \setcounter{enumi}{9}
    \item \textbf{Variabilidade da distribuição de caracteres} (\texttt{char\_entropy}): Medida de dispersão na distribuição de frequências de caracteres, calculada pela fórmula de Shannon $H = -\sum_{c} p(c) \log_2 p(c)$ \cite{shannon1948}, onde $p(c)$ é a probabilidade de ocorrência do caractere $c$.

    Esta medida quantifica a variabilidade: alta entropia indica distribuição mais uniforme (maior dispersão); baixa entropia indica concentração (menor dispersão).

    \textbf{Justificativa estatística}: Embora originalmente uma medida da teoria da informação, a entropia funciona como \textbf{medida de dispersão análoga ao desvio padrão}, mas aplicada a distribuições de frequência categórica. A entropia é mensurada em \textbf{escala de intervalo} porque:
    \begin{itemize}
        \item Diferenças entre valores são interpretáveis (aumento de 1 bit representa dobrar a incerteza)
        \item Não possui zero absoluto natural (zero ocorre apenas com um único caractere)
        \item Razões entre valores não são estatisticamente interpretáveis
    \end{itemize}
\end{enumerate}

\subsubsection{Justificativa da Escolha das Características}

Todas as características foram selecionadas por três critérios:

\begin{enumerate}
    \item \textbf{Objetividade}: Mensuração automática e determinística, sem julgamento subjetivo.
    \item \textbf{Robustez}: Insensibilidade a pequenas variações no texto ou erros de tokenização.
    \item \textbf{Fundamentação teórica}: Suporte empírico na literatura de estilometria para distinção de autoria.
\end{enumerate}

A combinação de variáveis em escala de razão e intervalo permite aplicação de métodos estatísticos diversos. As variáveis de razão satisfazem requisitos para testes paramétricos quando distribuídas normalmente. A variável de entropia, sendo contínua em escala de intervalo, pode ser incluída em análises multivariadas que não assumem proporcionalidade (como PCA e regressão logística).

\subsection{Testes Estatísticos Não Paramétricos}

Para cada característica, comparamos as distribuições entre textos autorais e de LLM usando o teste U de Mann--Whitney~\cite{mann1947}, um teste não paramétrico para duas amostras independentes. Testes não paramétricos foram escolhidos porque as características estilométricas frequentemente violam pressupostos de testes paramétricos: (i) as distribuições não são normais (verificado por inspeção visual e testes de normalidade), (ii) há presença de valores atípicos em dados linguísticos, e (iii) algumas características apresentam assimetria acentuada. O teste de Mann--Whitney compara as medianas das distribuições sem assumir forma distribucional específica. Calculamos valores-$p$ bicaudais para cada teste.

O tamanho de efeito foi quantificado pelo delta de Cliff ($\delta$)~\cite{cliff1993, cliff1996}, calculado como $\delta = (D^+ - D^-) / (n_1 \times n_2)$, onde $D^+$ é o número de pares em que valores do grupo autoral excedem valores do grupo LLM, $D^-$ é o número de pares na relação inversa, e $n_1$, $n_2$ são os tamanhos amostrais. O delta de Cliff admite interpretação probabilística: representa a diferença entre $P(X_{\text{autoral}} > X_{\text{LLM}})$ e $P(X_{\text{autoral}} < X_{\text{LLM}})$, estimando o grau de dominância estocástica de uma distribuição sobre a outra. O delta varia entre $-1$ e $+1$, onde valores próximos de zero indicam sobreposição completa entre as distribuições. Seguindo Romano et al.~\cite{romano2006}, interpretamos $|\delta| < 0.147$ como efeito negligenciável, $0.147 \leq |\delta| < 0.330$ como pequeno, $0.330 \leq |\delta| < 0.474$ como médio e $|\delta| \geq 0.474$ como grande.

Dado que realizamos testes múltiplos (10 características), aplicamos a correção de Benjamini--Hochberg~\cite{benjamini1995} para controlar a taxa de falsas descobertas (FDR). Esta correção fornece valores-$q$ ajustados, mantendo o controle sobre a proporção esperada de falsas rejeições entre todas as rejeições.

\subsection{Análise de Componentes Principais (PCA)}

Para visualizar a estrutura multivariada dos dados, aplicamos análise de componentes principais~\cite{jolliffe2002} às 10 características estilométricas. As variáveis foram previamente padronizadas (média zero, desvio padrão unitário) usando \texttt{StandardScaler} do scikit-learn~\cite{scikit-learn}. Retemos os dois primeiros componentes principais (PC1 e PC2) para visualização bidimensional. Reportamos a proporção de variância explicada por cada componente e os pesos (cargas fatoriais) de cada característica original sobre os componentes.

\subsection{Modelos de Classificação}

Avaliamos três modelos para classificação binária:

\begin{enumerate}
    \item \textbf{Análise Discriminante Linear (LDA):} um classificador generativo que assume distribuições Gaussianas multivariadas para cada classe com matrizes de covariância iguais, buscando a direção de projeção $w = S_W^{-1}(\mu_1 - \mu_2)$ que maximiza a separação entre classes~\cite{fisher1936, mclachlan2004}.

    \item \textbf{Regressão Logística:} um modelo discriminativo que estima diretamente a probabilidade posterior através da função logística $P(Y=1|X) = 1 / (1 + \exp(-(\beta_0 + \sum \beta_i x_i)))$, sem assumir normalidade das características~\cite{hosmer2013}.

    \item \textbf{Classificador Fuzzy:} um sistema baseado em regras com funções de pertinência triangulares orientadas por dados (definidas por quantis 33\%, 50\%, 66\%), agregação por média aritmética e inferência tipo Takagi-Sugeno ordem-zero. Detalhes completos em trabalho complementar sobre classificação fuzzy.
\end{enumerate}

Os modelos LDA e Regressão Logística foram treinados sobre as 10 características padronizadas (média zero, desvio padrão unitário). Para a regressão logística, utilizamos \texttt{max\_iter=1000} e sem regularização. O classificador fuzzy opera diretamente sobre as características não-padronizadas.

\subsection{Validação Cruzada e Métricas de Desempenho}

Empregamos validação cruzada estratificada com 5 partições (\texttt{StratifiedKFold}, \texttt{random\_state=42})~\cite{kohavi1995} para avaliar o desempenho dos classificadores. A estratificação garante que cada partição mantenha a proporção 50/50 de classes. Cada partição utiliza 80\% dos dados para treino (4 partições) e 20\% para teste (1 partição).

A métrica primária de avaliação é a \textbf{área sob a curva ROC (AUC)}~\cite{fawcett2006}, que resume a capacidade do modelo de discriminar entre as classes em todos os limiares de decisão. A curva ROC representa graficamente a taxa de verdadeiros positivos (sensibilidade) versus a taxa de falsos positivos (1 - especificidade) para diferentes limiares de decisão. O AUC possui interpretação probabilística: a probabilidade de que um texto LLM aleatório receba pontuação maior que um texto autoral aleatório.

Reportamos a média e o desvio padrão de AUC ao longo das 5 partições. Todas as análises foram implementadas em Python 3 utilizando as bibliotecas pandas~\cite{pandas}, NumPy~\cite{numpy}, scikit-learn~\cite{scikit-learn} e SciPy~\cite{scipy} para testes estatísticos.
