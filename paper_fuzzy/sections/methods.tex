% Fuzzy Set Theory Background and Methods

\subsection{Fundamentos de Conjuntos Fuzzy}

Um conjunto fuzzy $A$ em um universo de discurso $U$ caracteriza-se por uma função de pertinência $\mu_A: U \to [0,1]$ que atribui a cada elemento $x \in U$ um grau de pertinência no intervalo $[0,1]$~\cite{zadeh1965}. Diferentemente dos conjuntos clássicos, onde $\mu_A(x) \in \{0,1\}$, os conjuntos fuzzy permitem transições graduais quando a inclusão total ($\mu_A(x) = 1$) e exclusão total ($\mu_A(x) = 0$). Os conceitos a seguir são importantes:

\begin{itemize}
    \item \textbf{Núcleo (core):} $\{x \in U : \mu_A(x) = 1\}$, região com pertinência total
    \item \textbf{Suporte (support):} $\{x \in U : \mu_A(x) > 0\}$, região com pertinência não-nula
    \item \textbf{Fronteira (boundary):} região onde $0 < \mu_A(x) < 1$, pertinência parcial
\end{itemize}

A lógica fuzzy, proposta por Zadeh~\cite{zadeh1965}, estende a lógica Booleana clássica para manipular valores de verdade parciais e suportar raciocínio aproximado. Operações fundamentais incluem união (máximo), interseção (mínimo) e complemento ($1 - \mu_A(x)$).

\subsection{Conjunto de Dados}

Utilizou-se um corpus balanceado de textos em português do Brasil contendo 100.000 amostras (50.000 autorais, 50.000 de LLMs), extraídas por amostragem estratificada de um conjunto maior com 2.331.317 documentos originais provenientes de 5 fontes distintas. As fontes de texto autoral incluem: (i) BrWaC (Brazilian Web as Corpus), um grande corpus web de textos brasileiros; e (ii) BoolQ, contendo passagens de contexto para perguntas booleanas. As fontes de texto gerado por LLM incluem: (i) ShareGPT-Portuguese, conversas em português; (ii) resenhas do IMDB traduzidas para português por modelos de tradução automática; e (iii) o dataset Canarim, contendo saídas geradas por LLMs.

Os textos foram filtrados por comprimento mínimo de 100 caracteres e máximo de 10.000 caracteres, sendo textos muito longos segmentados em chunks de até 10.000 caracteres sem sobreposição. O balanceamento foi obtido por subamostragem da classe majoritária e sobreamostragem da classe minoritária, resultando em proporções exatamente iguais (50\%/50\%).

\subsection{Características Estilométricas}

Utilizamos 10 características estilométricas extraídas pelo módulo \texttt{src/features.py}, selecionadas por capturarem aspectos complementares da estrutura estatística e lexical dos textos: (1) \texttt{sent\_mean} -- comprimento médio de frase (tendência central); (2) \texttt{sent\_std} -- desvio padrão do comprimento de frase (dispersão sintática); (3) \texttt{sent\_burst} -- burstiness ($\sigma/\mu$, variabilidade relativa); (4) \texttt{ttr} -- relação tipo-token ($V/N$, diversidade lexical); (5) \texttt{herdan\_c} -- C de Herdan ($\log V / \log N$, diversidade normalizada); (6) \texttt{hapax\_prop} -- proporção de hapax legomena (raridade lexical); (7) \texttt{char\_entropy} -- entropia de Shannon sobre caracteres (variabilidade da distribuição); (8) \texttt{func\_word\_ratio} -- proporção de palavras funcionais (estabilidade lexical, menor variância entre textos); (9) \texttt{first\_person\_ratio} -- proporção de pronomes de primeira pessoa (subjetividade); e (10) \texttt{bigram\_repeat\_ratio} -- proporção de bigramas únicos repetidos (redundância local).

Do ponto de vista estatístico, todas as características são \textbf{variáveis contínuas}. Nove características estão em \textbf{escala de razão} (possuem zero absoluto e razões interpretáveis), enquanto a entropia de caracteres está em \textbf{escala de intervalo} (diferenças são interpretáveis mas razões não têm significado).

\subsection{Funções de Pertinência Triangulares}

Para cada característica, definimos três conjuntos fuzzy -- "baixo", "médio" e "alto" -- representados por funções de pertinência triangulares. Uma função triangular é determinada por três parâmetros $(a, b, c)$ e definida como:

\begin{equation}
\mu_{tri}(x; a, b, c) = \begin{cases}
0 & \text{se } x \leq a \text{ ou } x \geq c \\
\frac{x - a}{b - a} & \text{se } a < x < b \\
\frac{c - x}{c - b} & \text{se } b < x < c \\
1 & \text{se } x = b
\end{cases}
\end{equation}

As funções triangulares são amplamente utilizadas em sistemas fuzzy pela simplicidade computacional e pela facilidade de interpretação~\cite{wang1997}. Embora não sejam suaves nos vértices, fornecem aproximações satisfatórias para muitos problemas práticos.

\subsection{Determinação Orientada a Dados dos Parâmetros}

Ao invés de definir parâmetros manualmente, utilizamos uma abordagem \textbf{data-driven} baseada em quantis da distribuição observada dos dados de treinamento. Para cada característica $f_i$, calculamos:

\begin{itemize}
    \item Percentil 0\%: $q_0 = \min(f_i)$
    \item Percentil 33\%: $q_{33}$
    \item Percentil 50\%: $q_{50}$ (mediana)
    \item Percentil 66\%: $q_{66}$
    \item Percentil 100\%: $q_{100} = \max(f_i)$
\end{itemize}

As funções de pertinência são então definidas como:
\begin{align}
\mu_{low}(x) &= \mu_{tri}(x; q_0, q_{33}, q_{50}) \\
\mu_{medium}(x) &= \mu_{tri}(x; q_{33}, q_{50}, q_{66}) \\
\mu_{high}(x) &= \mu_{tri}(x; q_{50}, q_{66}, q_{100})
\end{align}

Esta abordagem garante que as funções de pertinência reflitam a distribuição empírica dos dados, adaptando-se automaticamente às características de cada métrica.

\subsection{Orientação e Regras Fuzzy}

Para determinar se valores altos ou baixos de uma característica indicam texto autoral, comparamos as medianas dos dois grupos (autoral e LLM):

\begin{itemize}
    \item Se $\text{mediana}_{\text{autoral}}(f_i) > \text{mediana}_{\text{LLM}}(f_i)$, a orientação é \textbf{direta}: valores altos $\to$ autoral
    \item Caso contrário, a orientação é \textbf{inversa}: valores baixos $\to$ autoral
\end{itemize}

Cada característica contribui com um ``voto'' para as hipóteses autoral ou LLM baseado no grau de pertinência. Por exemplo, para uma característica de orientação direta:
\begin{align}
\text{voto}_{\text{autoral}} &= \mu_{high}(x) + 0.5 \cdot \mu_{medium}(x) \\
\text{voto}_{\text{LLM}} &= \mu_{low}(x) + 0.5 \cdot \mu_{medium}(x)
\end{align}

Para orientação inversa, os papéis de ``high'' e ``low'' são invertidos. A pertinência média ($\mu_{medium}$) contribui igualmente para ambas as classes, refletindo incerteza.

\subsection{Sistema de Inferência e Classificação}

Para classificar um texto com características $(x_1, x_2, \ldots, x_{10})$, agregamos os votos de todas as características por média aritmética:

\begin{align}
S_{\text{autoral}} &= \frac{1}{10} \sum_{i=1}^{10} \text{voto}_{\text{autoral}}^{(i)} \\
S_{\text{LLM}} &= \frac{1}{10} \sum_{i=1}^{10} \text{voto}_{\text{LLM}}^{(i)}
\end{align}

Os scores são normalizados para fornecer probabilidades:
\begin{equation}
P(\text{autoral}) = \frac{S_{\text{autoral}}}{S_{\text{autoral}} + S_{\text{LLM}}}, \quad P(\text{LLM}) = \frac{S_{\text{LLM}}}{S_{\text{autoral}} + S_{\text{LLM}}}
\end{equation}

A classe predita é aquela com maior probabilidade. Este esquema de agregação simples corresponde a um sistema Takagi-Sugeno de ordem zero com pesos uniformes~\cite{takagi1985}. Operadores de agregação mais sofisticados (média ponderada, integrais fuzzy de Choquet ou Sugeno) poderiam ser explorados em trabalhos futuros.

\subsection{Validação Cruzada e Avaliação}

O classificador fuzzy é avaliado usando a mesma estratégia de validação cruzada estratificada (5 folds) empregada nos modelos estatísticos. Isso permite comparação direta e justa entre as abordagens. Para cada fold:

\begin{enumerate}
    \item As funções de pertinência são ajustadas nos dados de treinamento (determinação de quantis)
    \item A orientação de cada característica é determinada comparando medianas
    \item Predições são realizadas no conjunto de teste
    \item Métricas de desempenho são calculadas: ROC AUC e Average Precision
\end{enumerate}

Reportamos a média e o desvio padrão de AUC e AP ao longo dos 5 folds. A implementação foi realizada no módulo \texttt{src/fuzzy.py}, com avaliação via \texttt{src/evaluate\_fuzzy.py}.

\subsection{Vantagens da Abordagem Fuzzy}

A principal vantagem do classificador fuzzy é a \textbf{interpretabilidade}: os graus de pertinência podem ser inspecionados para compreender \textit{por que} um texto foi classificado como autoral ou LLM. Por exemplo, podemos visualizar:
\begin{itemize}
    \item Quais características contribuíram mais para a decisão
    \item Quão ``autoral-like'' ou ``LLM-like'' um texto é em cada dimensão
    \item Casos de incerteza (alto $\mu_{medium}$ em várias características)
\end{itemize}

Esta transparência contrasta com modelos de caixa-preta (redes neurais profundas, por exemplo) e facilita a análise qualitativa e a depuração do sistema. Além disso, a abordagem fuzzy permite incorporar conhecimento especializado linguístico na construção das funções de pertinência, embora neste trabalho tenhamos optado pela determinação automática via quantis.
