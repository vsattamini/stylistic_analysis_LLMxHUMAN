% Discussion

\subsection{Vantagens e Limitações da Abordagem Fuzzy}

O classificador fuzzy proposto alcançou desempenho sólido (89,34\% ROC AUC), embora inferior aos métodos estatísticos tradicionais (LDA: 94,12\%, Logística: 97,03\%). Esta diferença de ~8 pontos percentuais representa o \textbf{custo da interpretabilidade}: ao sacrificar complexidade algorítmica em favor de transparência e explicabilidade, aceitamos uma redução modesta no poder discriminatório.

Entretanto, esta perda é acompanhada de ganhos significativos:

\begin{itemize}
    \item \textbf{Robustez excepcional:} o desvio padrão do fuzzy ($\pm 0.04\%$) é 3--4× menor que os métodos estatísticos, indicando estabilidade superior a variações nos dados.

    \item \textbf{Interpretabilidade completa:} cada decisão pode ser decomposta em graus de pertinência por característica, permitindo auditoria e explicação detalhada.

    \item \textbf{Simplicidade computacional:} a classificação requer apenas 30 avaliações de funções triangulares (10 características × 3 conjuntos) e uma média, tornando o sistema extremamente eficiente.

    \item \textbf{Flexibilidade:} funções de pertinência podem ser ajustadas manualmente por especialistas linguísticos se conhecimento a priori estiver disponível.
\end{itemize}

As limitações principais da abordagem fuzzy incluem:

\begin{enumerate}
    \item \textbf{Simplicidade excessiva:} funções triangulares e agregação por média são escolhas básicas. Funções Gaussianas, trapezoidais ou bell-shaped, combinadas com operadores de agregação mais sofisticados (Choquet, Sugeno), poderiam melhorar o desempenho.

    \item \textbf{Independência de características:} o sistema atual trata cada característica independentemente, ignorando correlações. Regras fuzzy multi-dimensionais (e.g., ``SE ttr É alto E sent\_std É baixo ENTÃO llm'') poderiam capturar interações.

    \item \textbf{Orientação binária:} a estratégia de orientação (direta vs inversa) é binária. Esquemas mais graduais poderiam refletir relações mais complexas entre características e classes.

    \item \textbf{Pesos uniformes:} todas as características contribuem igualmente para a decisão final. Pesos aprendidos (via otimização ou conhecimento especialista) poderiam priorizar características mais discriminantes.
\end{enumerate}

\subsection{Comparação Fuzzy vs Métodos Estatísticos}

A Tabela de comparação revela padrões interessantes:

\begin{itemize}
    \item \textbf{Logística $>$ LDA $>$ Fuzzy:} hierarquia clara de desempenho, com diferenças consistentes de ~3\% e ~5\%.

    \item \textbf{Fuzzy tem menor variância:} $\sigma_{\text{fuzzy}} = 0.0004$ vs $\sigma_{\text{LDA}} = 0.0017$ vs $\sigma_{\text{logística}} = 0.0014$. Isto sugere que fuzzy é menos sensível à composição específica dos folds.

    \item \textbf{Trade-off favorável:} a perda de 7,9\% em AUC (de 97\% para 89\%) é compensada por explicabilidade total, uma troca que pode ser valiosa em contextos sensíveis.
\end{itemize}

Do ponto de vista de \textbf{aplicações práticas}, a escolha entre fuzzy e métodos estatísticos depende do contexto:

\begin{itemize}
    \item Se a prioridade é \textbf{máxima acurácia} (e.g., triagem automatizada em larga escala), \textbf{regressão logística} é superior.

    \item Se a prioridade é \textbf{explicabilidade} (e.g., decisões que precisam ser justificadas a usuários, auditoria de sistemas, contextos educacionais), \textbf{fuzzy} é preferível.

    \item Se deseja-se um \textbf{meio-termo} (boa acurácia com alguma interpretabilidade), \textbf{LDA} pode ser apropriado, embora ainda menos interpretável que fuzzy.
\end{itemize}

\subsection{Interpretação Linguística das Funções de Pertinência}

A visualização das funções de pertinência (Figura~\ref{fig:fuzzy_membership}) revela insights linguísticos:

\begin{itemize}
    \item \textbf{Entropia de caracteres:} a clara separação entre as distribuições humano/LLM nas regiões baixa/alta confirma que esta é a característica mais discriminante, um achado consistente com a análise estatística ($\delta = -0.881$).

    \item \textbf{TTR e hapax:} ambas mostram padrões similares (LLMs concentrados em valores altos), refletindo a forte correlação entre estas métricas ($r = 0.87$).

    \item \textbf{Sent\_std:} a sobreposição moderada entre distribuições explica o desempenho fuzzy -- há ambiguidade inerente que dificulta classificação baseada apenas nesta característica.
\end{itemize}

As funções de pertinência determinadas por quantis (33\%, 50\%, 66\%) capturam bem a estrutura dos dados, mas \textbf{não otimizam diretamente para separação de classes}. Abordagens futuras poderiam aprender thresholds discriminativos (e.g., via algoritmos genéticos, otimização por enxame de partículas) para maximizar AUC.

\subsection{Contribuição para a Literatura de Lógica Fuzzy}

Este trabalho é, ao nosso conhecimento, o \textbf{primeiro a aplicar lógica fuzzy para detecção de LLMs em qualquer língua}. Demonstramos que:

\begin{enumerate}
    \item Sistemas fuzzy simples (triangulares, agregação por média) já alcançam ~89\% AUC, um resultado competitivo.

    \item A abordagem data-driven (quantis) elimina a necessidade de definição manual de parâmetros, tornando o método escalável.

    \item A interpretabilidade fuzzy é particularmente valiosa para análise estilométrica, onde compreender \textit{por que} um texto foi classificado é tão importante quanto a classificação em si.
\end{enumerate}

Trabalhos futuros em lógica fuzzy para NLP podem se beneficiar destas lições, explorando:
- Funções de pertinência adaptativas que evoluem com novos dados
- Regras fuzzy hierárquicas que capturam interações entre características
- Sistemas neuro-fuzzy que combinam aprendizado profundo com interpretabilidade fuzzy

\subsection{Limitações e Trabalhos Futuros}

Além das limitações já mencionadas (simplicidade do sistema, independência de características), destacamos:

\begin{itemize}
    \item \textbf{Falta de validação em outros domínios:} testamos apenas em texto genérico em português. Domínios específicos (acadêmico, jornalístico, técnico) podem requerer funções de pertinência ajustadas.

    \item \textbf{Comparação limitada:} não comparamos contra sistemas fuzzy mais sofisticados (Mamdani, Larsen, TSK de ordem superior). Benchmarks mais amplos são necessários.

    \item \textbf{Ausência de análise de casos limite:} textos que recebem scores próximos de 50\%/50\% (alta incerteza) não foram analisados qualitativamente.
\end{itemize}

Direções futuras específicas para lógica fuzzy:
\begin{enumerate}
    \item Explorar operadores de agregação alternativos (Choquet integral, média ordenada ponderada)
    \item Implementar aprendizado de parâmetros fuzzy via otimização meta-heurística
    \item Desenvolver interfaces interativas onde usuários ajustam funções de pertinência em tempo real
    \item Aplicar sistemas fuzzy tipo-2 para modelar incerteza nas próprias funções de pertinência
\end{enumerate}
