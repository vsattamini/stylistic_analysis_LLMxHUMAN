{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ed6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ce690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json(\"data/sharegpt-portuguese.json\")\n",
    "\n",
    "df = df.drop(columns=[\"lang\"])\n",
    "source = []\n",
    "text = []\n",
    "ids = []\n",
    "# Rename 'gpt' labels to 'llm' for consistency\n",
    "source = ['llm' if label == 'gpt' else label for label in source]\n",
    "\n",
    "df = pd.DataFrame({'label': source, 'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d937f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13140c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"data/imdb-reviews-pt-br.csv\")\n",
    "df_1 = df_1.drop(columns=[\"id\", \"text_en\", \"sentiment\"])    \n",
    "df_1 = df_1.rename(columns={\"text_pt\": \"text\"})\n",
    "df_1['label'] = ['llm' for i in range(len(df_1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968b117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"data/boolq.csv\")\n",
    "label= []\n",
    "text = []\n",
    "for i, row in df_2.iterrows():\n",
    "    label.append('human')\n",
    "    text.append(row['passage'])\n",
    "df_2 = pd.DataFrame({'label': label, 'text': text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9514170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(\"data/validation_bool.csv\")\n",
    "label= []\n",
    "text = []\n",
    "for i, row in df_3.iterrows():\n",
    "    label.append('human')\n",
    "    text.append(row['passage'])\n",
    "df_3 = pd.DataFrame({'label': label, 'text': text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0208aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraphs_to_text(data_dict):\n",
    "    \"\"\"\n",
    "    Process data in the format {'paragraphs': [['text1'], ['text2'], ...]} \n",
    "    into a single text block.\n",
    "    \"\"\"\n",
    "    if 'paragraphs' not in data_dict:\n",
    "        raise ValueError(\"Data must contain 'paragraphs' key\")\n",
    "    \n",
    "    # Extract all text from paragraphs and join them\n",
    "    all_text = []\n",
    "    for paragraph in data_dict['paragraphs']:\n",
    "        # Each paragraph is a list, join its elements if there are multiple\n",
    "        if isinstance(paragraph, list):\n",
    "            paragraph_text = ' '.join(paragraph)\n",
    "        else:\n",
    "            paragraph_text = str(paragraph)\n",
    "        all_text.append(paragraph_text)\n",
    "    \n",
    "    # Join all paragraphs with double newlines for readability\n",
    "    return '\\n'.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing parquet files:  86%|████████▌ | 18/21 [07:35<01:16, 25.51s/it]"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Get all parquet files in the data folder\n",
    "parquet_files = glob.glob(\"data/brwac/*.parquet\")\n",
    "\n",
    "# Initialize lists outside the loop to avoid memory fragmentation\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for parquet_file in tqdm(parquet_files, desc=\"Processing parquet files\"):\n",
    "    # Read parquet file metadata to get row count\n",
    "    parquet_file_obj = pq.ParquetFile(parquet_file)\n",
    "    \n",
    "    # Process in batches using pyarrow\n",
    "    batch_size = 100000  # Adjust based on your available memory\n",
    "    for batch in parquet_file_obj.iter_batches(batch_size=batch_size):\n",
    "        # Convert batch to pandas DataFrame\n",
    "        df_batch = batch.to_pandas()\n",
    "        \n",
    "        # Process batch by batch instead of row by row\n",
    "        for i, row in df_batch.iterrows():\n",
    "            processed_text = process_paragraphs_to_text(row['text'])\n",
    "            text.append(processed_text)\n",
    "            label.append('human')\n",
    "        \n",
    "        # Clear batch from memory after processing\n",
    "        del df_batch, batch\n",
    "\n",
    "# Create DataFrame only once at the end\n",
    "df_4 = pd.DataFrame({'label': label, 'text': text})\n",
    "\n",
    "print(f\"Total labels: {len(label)}\")\n",
    "print(f\"Total texts: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee38d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "text = []\n",
    "parquet_files = glob.glob(\"data/canarim/*.parquet\")\n",
    "for parquet_file in parquet_files:\n",
    "    df_temp = pd.read_parquet(parquet_file)\n",
    "    for i, row in df_temp.iterrows():\n",
    "        labels.append('llm')\n",
    "        text.append(row['output'])\n",
    "df_5 = pd.DataFrame({'label': labels, 'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all 5 DataFrames, \n",
    "df_combined = pd.concat([df, df_1, df_2, df_3, df_4, df_5], ignore_index=True)\n",
    "\n",
    "# Describe the label distribution\n",
    "label_counts = df_combined['label'].value_counts()\n",
    "print(\"Label distribution:\")\n",
    "print(label_counts)\n",
    "print(f\"\\nTotal samples: {len(df_combined)}\")\n",
    "print(f\"Human samples: {label_counts.get('human', 0)} ({label_counts.get('human', 0)/len(df_combined)*100:.2f}%)\")\n",
    "print(f\"LLM samples: {label_counts.get('llm', 0)} ({label_counts.get('llm', 0)/len(df_combined)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65aac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Describe the label distribution\n",
    "label_counts = df_combined['label'].value_counts()\n",
    "print(\"Label distribution:\")\n",
    "print(label_counts)\n",
    "print(f\"\\nTotal samples: {len(df_combined)}\")\n",
    "print(f\"Human samples: {label_counts.get('human', 0)} ({label_counts.get('human', 0)/len(df_combined)*100:.2f}%)\")\n",
    "print(f\"LLM samples: {label_counts.get('llm', 0)} ({label_counts.get('llm', 0)/len(df_combined)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_balance(df, target_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Hybrid approach: downsample majority and upsample minority for better balance.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)} rows\")\n",
    "    print(\"Original label distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Separate human and LLM samples\n",
    "    human_samples = df[df['label'] == 'human']\n",
    "    llm_samples = df[df['label'] == 'llm']\n",
    "    \n",
    "    # Target size for each class\n",
    "    target_size = int((len(human_samples) + len(llm_samples)) * target_ratio)\n",
    "    \n",
    "    # Downsample human data\n",
    "    human_balanced = human_samples.sample(n=target_size)\n",
    "    \n",
    "    # Upsample LLM data\n",
    "    llm_upsampled = llm_samples.sample(n=target_size, replace=True, random_state=42)\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_balanced = pd.concat([human_balanced, llm_upsampled], ignore_index=True)\n",
    "    \n",
    "    # Shuffle the final dataset\n",
    "    df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset size: {len(df_balanced)} rows\")\n",
    "    print(\"Balanced label distribution:\")\n",
    "    print(df_balanced['label'].value_counts())\n",
    "    \n",
    "    # Save balanced dataset\n",
    "    df_balanced.to_csv(\"data/balanced.csv\", index=False)\n",
    "    print(f\"\\nBalanced dataset saved to data/balanced.csv\")\n",
    "\n",
    "# Usage\n",
    "hybrid_balance(df_combined, target_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e222958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2358f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(\"combined.csv\")\n",
    "#df_balanced = pd.read_csv(\"balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afea76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_chunk_text(df, min_length=200, max_length=10000, chunk_overlap=0):\n",
    "    \"\"\"\n",
    "    Filter out texts smaller than min_length and split texts longer than max_length into chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with 'text' and 'label' columns\n",
    "    - min_length: minimum character length to keep (default: 100)\n",
    "    - max_length: maximum character length before chunking (default: 10000)\n",
    "    - chunk_overlap: number of characters to overlap between chunks (default: 200)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with filtered and chunked texts\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)} rows\")\n",
    "    \n",
    "    # Filter out texts that are too short\n",
    "    df_filtered = df[df['text'].str.len() >= min_length].copy()\n",
    "    removed_short = len(df) - len(df_filtered)\n",
    "    print(f\"Removed {removed_short} entries shorter than {min_length} characters\")\n",
    "    \n",
    "    # Separate texts that need chunking from those that don't\n",
    "    df_normal = df_filtered[df_filtered['text'].str.len() <= max_length].copy()\n",
    "    df_to_chunk = df_filtered[df_filtered['text'].str.len() > max_length].copy()\n",
    "    \n",
    "    print(f\"Texts within normal range: {len(df_normal)}\")\n",
    "    print(f\"Texts requiring chunking: {len(df_to_chunk)}\")\n",
    "    \n",
    "    # Process chunked texts\n",
    "    chunked_rows = []\n",
    "    total_chunks = 0\n",
    "    \n",
    "    for idx, row in df_to_chunk.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = create_text_chunks(text, max_length, chunk_overlap)\n",
    "        total_chunks += len(chunks)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunked_rows.append({\n",
    "                'text': chunk,\n",
    "                'label': label,\n",
    "                'original_length': len(text),\n",
    "                'chunk_id': f\"{idx}_{len(chunked_rows)}\"\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from chunked data\n",
    "    if chunked_rows:\n",
    "        df_chunked = pd.DataFrame(chunked_rows)\n",
    "        # Combine normal and chunked data\n",
    "        df_final = pd.concat([\n",
    "            df_normal.assign(original_length=df_normal['text'].str.len(), chunk_id=''),\n",
    "            df_chunked\n",
    "        ], ignore_index=True)\n",
    "    else:\n",
    "        df_final = df_normal.assign(original_length=df_normal['text'].str.len(), chunk_id='')\n",
    "    \n",
    "    print(f\"Final dataset size: {len(df_final)} rows\")\n",
    "    print(f\"Total chunks created from long texts: {total_chunks}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def create_text_chunks(text, max_length, overlap):\n",
    "    \"\"\"\n",
    "    Split text into chunks with specified overlap.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: input text to chunk\n",
    "    - max_length: maximum length of each chunk\n",
    "    - overlap: number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "    - List of text chunks\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Calculate end position\n",
    "        end = start + max_length\n",
    "        \n",
    "        # If this is not the last chunk, try to break at a good point\n",
    "        if end < len(text):\n",
    "            # Look for a good breaking point (sentence end, paragraph, or space)\n",
    "            break_points = ['. ', '.\\n', '\\n\\n', ' ']\n",
    "            best_break = end\n",
    "            \n",
    "            for break_char in break_points:\n",
    "                # Look backward from the end position for a good break\n",
    "                last_break = text.rfind(break_char, start, end)\n",
    "                if last_break > start + max_length // 2:  # Don't break too early\n",
    "                    best_break = last_break + len(break_char)\n",
    "                    break\n",
    "            \n",
    "            chunk = text[start:best_break].strip()\n",
    "        else:\n",
    "            # Last chunk - take the rest\n",
    "            chunk = text[start:].strip()\n",
    "        \n",
    "        if chunk:  # Only add non-empty chunks\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Move start position with overlap\n",
    "        if end >= len(text):\n",
    "            break\n",
    "        start = max(start + max_length - overlap, best_break - overlap)\n",
    "        \n",
    "        # Ensure we're making progress\n",
    "        if start <= 0:\n",
    "            start = max_length - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06883d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filtering and chunking to df_combined...\n",
      "Original dataset size: 2331317 rows\n",
      "Removed 171510 entries shorter than 100 characters\n",
      "Texts within normal range: 1992995\n",
      "Texts requiring chunking: 166812\n"
     ]
    }
   ],
   "source": [
    "# Apply filtering and chunking to the combined dataset\n",
    "print(\"Applying filtering and chunking to df_combined...\")\n",
    "df_processed = filter_and_chunk_text(df_combined, min_length=100, max_length=10000, chunk_overlap=0)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Length distribution analysis\n",
    "length_stats = df_processed['text'].str.len().describe()\n",
    "print(f\"\\nText length statistics after processing:\")\n",
    "print(length_stats)\n",
    "\n",
    "# Label distribution\n",
    "label_counts = df_processed['label'].value_counts()\n",
    "print(f\"\\nLabel distribution after processing:\")\n",
    "print(label_counts)\n",
    "print(f\"Human samples: {label_counts.get('human', 0)} ({label_counts.get('human', 0)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"LLM samples: {label_counts.get('llm', 0)} ({label_counts.get('llm', 0)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# Chunked entries analysis\n",
    "chunked_entries = df_processed[df_processed['chunk_id'] != '']\n",
    "print(f\"\\nChunked entries: {len(chunked_entries)}\")\n",
    "if len(chunked_entries) > 0:\n",
    "    print(f\"Average original length of chunked texts: {chunked_entries['original_length'].mean():.0f} chars\")\n",
    "    print(f\"Max original length: {chunked_entries['original_length'].max():.0f} chars\")\n",
    "    print(f\"Unique original texts that were chunked: {chunked_entries['original_length'].nunique()}\")\n",
    "\n",
    "# Save processed dataset\n",
    "df_processed.to_csv(\"processed_filtered_chunked.csv\", index=False)\n",
    "print(f\"\\nProcessed dataset saved to data/processed_filtered_chunked.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis functions for the filtered and chunked data\n",
    "\n",
    "def analyze_chunking_results(df):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis of the chunking results.\n",
    "    \"\"\"\n",
    "    print(\"DETAILED CHUNKING ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = len(df)\n",
    "    chunked_rows = len(df[df['chunk_id'] != ''])\n",
    "    original_rows = len(df[df['chunk_id'] == ''])\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Original (non-chunked) rows: {original_rows}\")\n",
    "    print(f\"Chunked rows: {chunked_rows}\")\n",
    "    print(f\"Chunking ratio: {chunked_rows/total_rows*100:.2f}%\")\n",
    "    \n",
    "    # Length distribution\n",
    "    print(f\"\\nLength distribution:\")\n",
    "    print(f\"Min length: {df['text'].str.len().min()}\")\n",
    "    print(f\"Max length: {df['text'].str.len().max()}\")\n",
    "    print(f\"Mean length: {df['text'].str.len().mean():.1f}\")\n",
    "    print(f\"Median length: {df['text'].str.len().median():.1f}\")\n",
    "    \n",
    "    # Chunked vs original comparison by label\n",
    "    print(f\"\\nBreakdown by label:\")\n",
    "    for label in df['label'].unique():\n",
    "        label_data = df[df['label'] == label]\n",
    "        label_chunked = len(label_data[label_data['chunk_id'] != ''])\n",
    "        label_original = len(label_data[label_data['chunk_id'] == ''])\n",
    "        print(f\"  {label.upper()}:\")\n",
    "        print(f\"    Original entries: {label_original}\")\n",
    "        print(f\"    Chunked entries: {label_chunked}\")\n",
    "        print(f\"    Chunking ratio: {label_chunked/(label_chunked + label_original)*100:.2f}%\")\n",
    "\n",
    "def create_balanced_chunked_dataset(df, target_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset from the filtered and chunked data.\n",
    "    \"\"\"\n",
    "    print(\"CREATING BALANCED DATASET FROM CHUNKED DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Separate by label\n",
    "    human_data = df[df['label'] == 'human']\n",
    "    llm_data = df[df['label'] == 'llm']\n",
    "    \n",
    "    print(f\"Human entries: {len(human_data)}\")\n",
    "    print(f\"LLM entries: {len(llm_data)}\")\n",
    "    \n",
    "    # Calculate target sizes\n",
    "    total_target = int((len(human_data) + len(llm_data)) * target_ratio)\n",
    "    \n",
    "    # Sample from each class\n",
    "    human_sampled = human_data.sample(n=min(total_target, len(human_data)), random_state=42)\n",
    "    llm_sampled = llm_data.sample(n=min(total_target, len(llm_data)), replace=len(llm_data) < total_target, random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([human_sampled, llm_sampled], ignore_index=True)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset created:\")\n",
    "    print(f\"Total size: {len(balanced_df)}\")\n",
    "    print(balanced_df['label'].value_counts())\n",
    "    \n",
    "    # Save balanced dataset\n",
    "    balanced_df.to_csv(\"    .csv\", index=False)\n",
    "    print(f\"\\nBalanced dataset saved to data/balanced_filtered_chunked.csv\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# You can call these functions after running the filtering and chunking\n",
    "# analyze_chunking_results(df_processed)\n",
    "# df_balanced_chunked = create_balanced_chunked_dataset(df_processed, target_ratio=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dcbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def filter_and_chunk_text_batch(input_file=\"combined.csv\", output_file=\"processed_filtered_chunked.csv\", \n",
    "                                min_length=200, max_length=10000, chunk_overlap=0, \n",
    "                                batch_size=50000, intermediate_save_every=5):\n",
    "    \"\"\"\n",
    "    Memory-efficient batch processing version of filter_and_chunk_text.\n",
    "    Processes data in batches and saves intermediate results.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: path to input CSV file\n",
    "    - output_file: path to output CSV file\n",
    "    - min_length: minimum character length to keep (default: 200)\n",
    "    - max_length: maximum character length before chunking (default: 10000)\n",
    "    - chunk_overlap: number of characters to overlap between chunks (default: 0)\n",
    "    - batch_size: number of rows to process at once (default: 50000)\n",
    "    - intermediate_save_every: save intermediate results every N batches (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    - None (saves results to file)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting batch processing of {input_file}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Intermediate saves every {intermediate_save_every} batches\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    total_processed = 0\n",
    "    total_removed_short = 0\n",
    "    total_normal = 0\n",
    "    total_chunked = 0\n",
    "    total_chunks_created = 0\n",
    "    batch_number = 0\n",
    "    \n",
    "    # Create intermediate directory if it doesn't exist\n",
    "    intermediate_dir = \"intermediate_results\"\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "    \n",
    "    # Read file in batches\n",
    "    chunk_iter = pd.read_csv(input_file, chunksize=batch_size)\n",
    "    \n",
    "    processed_batches = []\n",
    "    \n",
    "    for batch_df in tqdm(chunk_iter, desc=\"Processing batches\"):\n",
    "        batch_number += 1\n",
    "        print(f\"\\nProcessing batch {batch_number} with {len(batch_df)} rows\")\n",
    "        \n",
    "        # Process current batch\n",
    "        batch_result = process_single_batch(batch_df, min_length, max_length, chunk_overlap)\n",
    "        \n",
    "        # Update counters\n",
    "        batch_stats = batch_result['stats']\n",
    "        total_processed += batch_stats['original_size']\n",
    "        total_removed_short += batch_stats['removed_short']\n",
    "        total_normal += batch_stats['normal_texts']\n",
    "        total_chunked += batch_stats['chunked_texts']\n",
    "        total_chunks_created += batch_stats['chunks_created']\n",
    "        \n",
    "        # Store processed batch\n",
    "        processed_batches.append(batch_result['data'])\n",
    "        \n",
    "        # Intermediate save\n",
    "        if batch_number % intermediate_save_every == 0:\n",
    "            print(f\"Saving intermediate results after batch {batch_number}...\")\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"processed_batch_{batch_number}.csv\")\n",
    "            \n",
    "            # Combine all processed batches so far\n",
    "            combined_batch = pd.concat(processed_batches, ignore_index=True)\n",
    "            combined_batch.to_csv(intermediate_file, index=False)\n",
    "            \n",
    "            print(f\"Intermediate results saved to {intermediate_file}\")\n",
    "            print(f\"Rows processed so far: {len(combined_batch)}\")\n",
    "            \n",
    "            # Clear processed batches from memory and force garbage collection\n",
    "            del combined_batch\n",
    "            processed_batches = []\n",
    "            gc.collect()\n",
    "    \n",
    "    # Final save - combine any remaining batches\n",
    "    if processed_batches:\n",
    "        print(\"\\nSaving final batch...\")\n",
    "        final_batch = pd.concat(processed_batches, ignore_index=True)\n",
    "        \n",
    "        # If there are intermediate files, combine them with the final batch\n",
    "        intermediate_files = [f for f in os.listdir(intermediate_dir) if f.startswith(\"processed_batch_\")]\n",
    "        \n",
    "        if intermediate_files:\n",
    "            print(\"Combining with intermediate results...\")\n",
    "            all_dfs = [final_batch]\n",
    "            \n",
    "            for int_file in sorted(intermediate_files):\n",
    "                int_path = os.path.join(intermediate_dir, int_file)\n",
    "                int_df = pd.read_csv(int_path)\n",
    "                all_dfs.append(int_df)\n",
    "            \n",
    "            final_result = pd.concat(all_dfs, ignore_index=True)\n",
    "        else:\n",
    "            final_result = final_batch\n",
    "        \n",
    "        # Save final result\n",
    "        final_result.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Clean up\n",
    "        del final_batch, final_result\n",
    "        if intermediate_files:\n",
    "            for int_file in intermediate_files:\n",
    "                os.remove(os.path.join(intermediate_dir, int_file))\n",
    "    else:\n",
    "        # Only intermediate files exist\n",
    "        intermediate_files = [f for f in os.listdir(intermediate_dir) if f.startswith(\"processed_batch_\")]\n",
    "        if intermediate_files:\n",
    "            print(\"Combining intermediate results...\")\n",
    "            all_dfs = []\n",
    "            \n",
    "            for int_file in sorted(intermediate_files):\n",
    "                int_path = os.path.join(intermediate_dir, int_file)\n",
    "                int_df = pd.read_csv(int_path)\n",
    "                all_dfs.append(int_df)\n",
    "            \n",
    "            final_result = pd.concat(all_dfs, ignore_index=True)\n",
    "            final_result.to_csv(output_file, index=False)\n",
    "            \n",
    "            # Clean up\n",
    "            for int_file in intermediate_files:\n",
    "                os.remove(os.path.join(intermediate_dir, int_file))\n",
    "    \n",
    "    # Remove intermediate directory if empty\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BATCH PROCESSING COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total rows processed: {total_processed}\")\n",
    "    print(f\"Removed short texts: {total_removed_short}\")\n",
    "    print(f\"Normal texts: {total_normal}\")\n",
    "    print(f\"Texts requiring chunking: {total_chunked}\")\n",
    "    print(f\"Total chunks created: {total_chunks_created}\")\n",
    "    print(f\"Final dataset saved to: {output_file}\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def process_single_batch(batch_df, min_length, max_length, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Process a single batch of data with filtering and chunking.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_df: DataFrame batch to process\n",
    "    - min_length, max_length, chunk_overlap: processing parameters\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with processed data and statistics\n",
    "    \"\"\"\n",
    "    original_size = len(batch_df)\n",
    "    \n",
    "    # Filter out texts that are too short\n",
    "    batch_filtered = batch_df[batch_df['text'].str.len() >= min_length].copy()\n",
    "    removed_short = original_size - len(batch_filtered)\n",
    "    \n",
    "    # Separate texts that need chunking from those that don't\n",
    "    batch_normal = batch_filtered[batch_filtered['text'].str.len() <= max_length].copy()\n",
    "    batch_to_chunk = batch_filtered[batch_filtered['text'].str.len() > max_length].copy()\n",
    "    \n",
    "    # Process chunked texts\n",
    "    chunked_rows = []\n",
    "    chunks_created = 0\n",
    "    \n",
    "    for idx, row in batch_to_chunk.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = create_text_chunks_efficient(text, max_length, chunk_overlap)\n",
    "        chunks_created += len(chunks)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_rows.append({\n",
    "                'text': chunk,\n",
    "                'label': label,\n",
    "                'original_length': len(text),\n",
    "                'chunk_id': f\"{idx}_{i}\"\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from chunked data\n",
    "    if chunked_rows:\n",
    "        batch_chunked = pd.DataFrame(chunked_rows)\n",
    "        # Combine normal and chunked data\n",
    "        batch_result = pd.concat([\n",
    "            batch_normal.assign(original_length=batch_normal['text'].str.len(), chunk_id=''),\n",
    "            batch_chunked\n",
    "        ], ignore_index=True)\n",
    "    else:\n",
    "        batch_result = batch_normal.assign(original_length=batch_normal['text'].str.len(), chunk_id='')\n",
    "    \n",
    "    # Statistics for this batch\n",
    "    stats = {\n",
    "        'original_size': original_size,\n",
    "        'removed_short': removed_short,\n",
    "        'normal_texts': len(batch_normal),\n",
    "        'chunked_texts': len(batch_to_chunk),\n",
    "        'chunks_created': chunks_created,\n",
    "        'final_size': len(batch_result)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'data': batch_result,\n",
    "        'stats': stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6dc727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_chunks_efficient(text, max_length, overlap):\n",
    "    \"\"\"\n",
    "    Memory-efficient version of create_text_chunks.\n",
    "    Uses generators and minimal memory allocation.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: input text to chunk\n",
    "    - max_length: maximum length of each chunk\n",
    "    - overlap: number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "    - List of text chunks\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # Pre-define break points for efficiency\n",
    "    break_points = ['. ', '.\\n', '\\n\\n', ' ']\n",
    "    half_max = max_length // 2\n",
    "    \n",
    "    while start < text_length:\n",
    "        # Calculate end position\n",
    "        end = min(start + max_length, text_length)\n",
    "        \n",
    "        # If this is not the last chunk, try to break at a good point\n",
    "        if end < text_length:\n",
    "            best_break = end\n",
    "            \n",
    "            # Use more efficient searching\n",
    "            for break_char in break_points:\n",
    "                break_char_len = len(break_char)\n",
    "                # Look backward from the end position for a good break\n",
    "                search_start = max(start + half_max, start)\n",
    "                last_break = text.rfind(break_char, search_start, end)\n",
    "                \n",
    "                if last_break != -1:  # Found a good break point\n",
    "                    best_break = last_break + break_char_len\n",
    "                    break\n",
    "            \n",
    "            # Extract chunk efficiently\n",
    "            chunk = text[start:best_break].strip()\n",
    "            next_start = max(start + max_length - overlap, best_break - overlap)\n",
    "        else:\n",
    "            # Last chunk - take the rest\n",
    "            chunk = text[start:].strip()\n",
    "            next_start = text_length  # This will end the loop\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Update start position and ensure progress\n",
    "        if next_start <= start:\n",
    "            next_start = start + max_length - overlap\n",
    "        start = next_start\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def estimate_memory_usage(input_file=\"combined.csv\", batch_size=50000):\n",
    "    \"\"\"\n",
    "    Estimate memory usage and provide recommendations for batch processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: path to input CSV file\n",
    "    - batch_size: proposed batch size\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with memory estimates and recommendations\n",
    "    \"\"\"\n",
    "    print(\"Estimating memory usage...\")\n",
    "    \n",
    "    # Read a small sample to estimate row size\n",
    "    sample_df = pd.read_csv(input_file, nrows=1000)\n",
    "    \n",
    "    # Calculate average row size in memory\n",
    "    memory_usage = sample_df.memory_usage(deep=True).sum()\n",
    "    avg_row_size = memory_usage / len(sample_df)\n",
    "    \n",
    "    # Estimate total file size\n",
    "    total_rows = sum(1 for _ in open(input_file)) - 1  # Subtract header\n",
    "    estimated_total_memory = avg_row_size * total_rows\n",
    "    \n",
    "    # Estimate batch memory usage\n",
    "    batch_memory = avg_row_size * batch_size\n",
    "    \n",
    "    # Calculate chunking overhead (estimated 2x increase due to chunking)\n",
    "    chunking_overhead = 2.0\n",
    "    estimated_batch_memory_with_overhead = batch_memory * chunking_overhead\n",
    "    \n",
    "    print(f\"\\nMEMORY USAGE ESTIMATES\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total rows in file: {total_rows:,}\")\n",
    "    print(f\"Average row size: {avg_row_size:.2f} bytes\")\n",
    "    print(f\"Estimated total file memory: {estimated_total_memory / (1024**2):.1f} MB\")\n",
    "    print(f\"Batch size: {batch_size:,} rows\")\n",
    "    print(f\"Estimated batch memory: {batch_memory / (1024**2):.1f} MB\")\n",
    "    print(f\"Est. batch memory with chunking: {estimated_batch_memory_with_overhead / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations = []\n",
    "    if estimated_batch_memory_with_overhead > 2 * 1024**3:  # > 2GB\n",
    "        recommendations.append(\"Consider reducing batch_size to 25000 or less\")\n",
    "    if estimated_batch_memory_with_overhead > 8 * 1024**3:  # > 8GB\n",
    "        recommendations.append(\"WARNING: Batch size may cause out-of-memory errors\")\n",
    "    \n",
    "    if recommendations:\n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        for rec in recommendations:\n",
    "            print(f\"- {rec}\")\n",
    "    else:\n",
    "        print(f\"\\nBatch size looks good for memory usage!\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'avg_row_size': avg_row_size,\n",
    "        'estimated_total_memory_mb': estimated_total_memory / (1024**2),\n",
    "        'estimated_batch_memory_mb': estimated_batch_memory_with_overhead / (1024**2),\n",
    "        'recommendations': recommendations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1070493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating memory usage for combined.csv...\n",
      "Estimating memory usage...\n",
      "\n",
      "MEMORY USAGE ESTIMATES\n",
      "========================================\n",
      "Total rows in file: 71,822,334\n",
      "Average row size: 1520.40 bytes\n",
      "Estimated total file memory: 104139.8 MB\n",
      "Batch size: 50,000 rows\n",
      "Estimated batch memory: 72.5 MB\n",
      "Est. batch memory with chunking: 145.0 MB\n",
      "\n",
      "Batch size looks good for memory usage!\n",
      "\n",
      "============================================================\n",
      "Starting memory-efficient batch processing...\n",
      "============================================================\n",
      "Starting batch processing of combined.csv\n",
      "Batch size: 50000\n",
      "Intermediate saves every 3 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 1it [00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1 with 50000 rows\n",
      "\n",
      "Processing batch 2 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 2it [00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 3 with 50000 rows\n",
      "Saving intermediate results after batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 3it [00:09,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_3.csv\n",
      "Rows processed so far: 168970\n",
      "\n",
      "Processing batch 4 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 4it [00:10,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 5 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 5it [00:12,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 6 with 50000 rows\n",
      "Saving intermediate results after batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 6it [00:19,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_6.csv\n",
      "Rows processed so far: 173131\n",
      "\n",
      "Processing batch 7 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 8it [00:22,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 8 with 50000 rows\n",
      "\n",
      "Processing batch 9 with 50000 rows\n",
      "Saving intermediate results after batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 9it [00:30,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_9.csv\n",
      "Rows processed so far: 174971\n",
      "\n",
      "Processing batch 10 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 10it [00:32,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 11 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 11it [00:34,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 12 with 50000 rows\n",
      "Saving intermediate results after batch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 12it [00:42,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_12.csv\n",
      "Rows processed so far: 180324\n",
      "\n",
      "Processing batch 13 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 13it [00:44,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 14 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 14it [00:45,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 15 with 50000 rows\n",
      "Saving intermediate results after batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 15it [00:54,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_15.csv\n",
      "Rows processed so far: 178472\n",
      "\n",
      "Processing batch 16 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 16it [00:56,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 17 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 17it [00:58,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 18 with 50000 rows\n",
      "Saving intermediate results after batch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 18it [01:06,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_18.csv\n",
      "Rows processed so far: 179664\n",
      "\n",
      "Processing batch 19 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 19it [01:07,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 20 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 20it [01:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 21 with 50000 rows\n",
      "Saving intermediate results after batch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 21it [01:16,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_21.csv\n",
      "Rows processed so far: 173778\n",
      "\n",
      "Processing batch 22 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 22it [01:18,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 23 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 23it [01:19,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 24 with 50000 rows\n",
      "Saving intermediate results after batch 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 24it [01:28,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_24.csv\n",
      "Rows processed so far: 179550\n",
      "\n",
      "Processing batch 25 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 25it [01:30,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 26 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 26it [01:33,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 27 with 50000 rows\n",
      "Saving intermediate results after batch 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 27it [01:43,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_27.csv\n",
      "Rows processed so far: 198815\n",
      "\n",
      "Processing batch 28 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 28it [01:45,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 29 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 29it [01:47,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 30 with 50000 rows\n",
      "Saving intermediate results after batch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 30it [01:56,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_30.csv\n",
      "Rows processed so far: 183798\n",
      "\n",
      "Processing batch 31 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 31it [01:57,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 32 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 32it [01:59,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 33 with 50000 rows\n",
      "Saving intermediate results after batch 33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 33it [02:06,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_33.csv\n",
      "Rows processed so far: 173071\n",
      "\n",
      "Processing batch 34 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 34it [02:07,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 35 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 35it [02:09,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 36 with 50000 rows\n",
      "Saving intermediate results after batch 36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 36it [02:18,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_36.csv\n",
      "Rows processed so far: 183387\n",
      "\n",
      "Processing batch 37 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 37it [02:20,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 38 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 38it [02:22,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 39 with 50000 rows\n",
      "Saving intermediate results after batch 39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 39it [02:32,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_39.csv\n",
      "Rows processed so far: 187793\n",
      "\n",
      "Processing batch 40 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 41it [02:34,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 41 with 50000 rows\n",
      "\n",
      "Processing batch 42 with 50000 rows\n",
      "Saving intermediate results after batch 42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 42it [02:39,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_42.csv\n",
      "Rows processed so far: 142089\n",
      "\n",
      "Processing batch 43 with 50000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 43it [02:39,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 44 with 50000 rows\n",
      "\n",
      "Processing batch 45 with 50000 rows\n",
      "Saving intermediate results after batch 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 47it [02:39,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to intermediate_results/processed_batch_45.csv\n",
      "Rows processed so far: 66266\n",
      "\n",
      "Processing batch 46 with 50000 rows\n",
      "\n",
      "Processing batch 47 with 31317 rows\n",
      "\n",
      "Saving final batch...\n",
      "Combining with intermediate results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETED\n",
      "============================================================\n",
      "Total rows processed: 2331317\n",
      "Removed short texts: 171510\n",
      "Normal texts: 1992995\n",
      "Texts requiring chunking: 166812\n",
      "Total chunks created: 567293\n",
      "Final dataset saved to: processed_filtered_chunked_batch.csv\n"
     ]
    }
   ],
   "source": [
    "# Test the memory-efficient batch processing\n",
    "# First, let's estimate memory usage\n",
    "print(\"Estimating memory usage for combined.csv...\")\n",
    "memory_info = estimate_memory_usage(\"combined.csv\", batch_size=50000)\n",
    "\n",
    "# Run the batch processing with appropriate parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting memory-efficient batch processing...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Adjust batch size based on memory estimates if needed\n",
    "recommended_batch_size = 50000\n",
    "if memory_info['estimated_batch_memory_mb'] > 1000:  # > 1GB\n",
    "    recommended_batch_size = 25000\n",
    "    print(f\"Reducing batch size to {recommended_batch_size} due to memory constraints\")\n",
    "\n",
    "# Run the batch processing\n",
    "filter_and_chunk_text_batch(\n",
    "    input_file=\"combined.csv\",\n",
    "    output_file=\"processed_filtered_chunked_batch.csv\",\n",
    "    min_length=100,\n",
    "    max_length=10000,\n",
    "    chunk_overlap=0,\n",
    "    batch_size=recommended_batch_size,\n",
    "    intermediate_save_every=3  # Save every 3 batches to avoid losing progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_processed_results(processed_file=\"processed_filtered_chunked_batch.csv\"):\n",
    "    \"\"\"\n",
    "    Analyze the results of batch processing without loading the entire dataset into memory.\n",
    "    \n",
    "    Parameters:\n",
    "    - processed_file: path to the processed CSV file\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing processed results from {processed_file}...\")\n",
    "    \n",
    "    # Read file in small chunks to analyze without loading everything\n",
    "    chunk_size = 10000\n",
    "    total_rows = 0\n",
    "    label_counts = {'human': 0, 'llm': 0}\n",
    "    chunk_counts = 0\n",
    "    length_stats = []\n",
    "    \n",
    "    chunk_iter = pd.read_csv(processed_file, chunksize=chunk_size)\n",
    "    \n",
    "    for chunk in tqdm(chunk_iter, desc=\"Analyzing chunks\"):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Count labels\n",
    "        chunk_label_counts = chunk['label'].value_counts()\n",
    "        for label, count in chunk_label_counts.items():\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += count\n",
    "        \n",
    "        # Count chunked entries\n",
    "        chunk_counts += len(chunk[chunk['chunk_id'] != ''])\n",
    "        \n",
    "        # Sample length statistics\n",
    "        if len(length_stats) < 50000:  # Collect sample for statistics\n",
    "            length_stats.extend(chunk['text'].str.len().tolist())\n",
    "    \n",
    "    # Calculate statistics\n",
    "    length_stats = pd.Series(length_stats[:50000])  # Limit for memory\n",
    "    \n",
    "    print(f\"\\nPROCESSED DATA ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Chunked entries: {chunk_counts:,} ({chunk_counts/total_rows*100:.2f}%)\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  {label}: {count:,} ({count/total_rows*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nText length statistics (sample):\")\n",
    "    print(f\"  Min: {length_stats.min()}\")\n",
    "    print(f\"  Max: {length_stats.max()}\")\n",
    "    print(f\"  Mean: {length_stats.mean():.1f}\")\n",
    "    print(f\"  Median: {length_stats.median():.1f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'label_counts': label_counts,\n",
    "        'chunk_counts': chunk_counts,\n",
    "        'length_stats': {\n",
    "            'min': length_stats.min(),\n",
    "            'max': length_stats.max(),\n",
    "            'mean': length_stats.mean(),\n",
    "            'median': length_stats.median()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def create_balanced_dataset_batch(input_file=\"processed_filtered_chunked_batch.csv\", \n",
    "                                 output_file=\"balanced_processed.csv\",\n",
    "                                 target_ratio=0.3, batch_size=25000):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset from the processed file using batch processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: path to processed CSV file\n",
    "    - output_file: path for balanced output\n",
    "    - target_ratio: target ratio for balancing\n",
    "    - batch_size: batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "    - None (saves balanced dataset to file)\n",
    "    \"\"\"\n",
    "    print(f\"Creating balanced dataset from {input_file}...\")\n",
    "    \n",
    "    # First pass: count labels\n",
    "    label_counts = {'human': 0, 'llm': 0}\n",
    "    chunk_iter = pd.read_csv(input_file, chunksize=batch_size)\n",
    "    \n",
    "    for chunk in tqdm(chunk_iter, desc=\"Counting labels\"):\n",
    "        chunk_label_counts = chunk['label'].value_counts()\n",
    "        for label, count in chunk_label_counts.items():\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += count\n",
    "    \n",
    "    print(f\"Label counts: {label_counts}\")\n",
    "    \n",
    "    # Calculate target sizes\n",
    "    total_target = int((label_counts['human'] + label_counts['llm']) * target_ratio)\n",
    "    target_per_label = total_target // 2\n",
    "    \n",
    "    print(f\"Target size per label: {target_per_label:,}\")\n",
    "    \n",
    "    # Second pass: sample data\n",
    "    human_sampled = []\n",
    "    llm_sampled = []\n",
    "    human_count = 0\n",
    "    llm_count = 0\n",
    "    \n",
    "    chunk_iter = pd.read_csv(input_file, chunksize=batch_size)\n",
    "    \n",
    "    for chunk in tqdm(chunk_iter, desc=\"Sampling data\"):\n",
    "        # Sample humans\n",
    "        human_chunk = chunk[chunk['label'] == 'human']\n",
    "        if len(human_chunk) > 0 and human_count < target_per_label:\n",
    "            needed = min(len(human_chunk), target_per_label - human_count)\n",
    "            sampled = human_chunk.sample(n=needed, random_state=42)\n",
    "            human_sampled.append(sampled)\n",
    "            human_count += len(sampled)\n",
    "        \n",
    "        # Sample LLMs (with replacement if needed)\n",
    "        llm_chunk = chunk[chunk['label'] == 'llm']\n",
    "        if len(llm_chunk) > 0 and llm_count < target_per_label:\n",
    "            needed = min(len(llm_chunk), target_per_label - llm_count)\n",
    "            # Use replacement if we need more samples than available\n",
    "            replace = needed > len(llm_chunk)\n",
    "            sampled = llm_chunk.sample(n=needed, replace=replace, random_state=42)\n",
    "            llm_sampled.append(sampled)\n",
    "            llm_count += len(sampled)\n",
    "        \n",
    "        # Stop if we have enough samples\n",
    "        if human_count >= target_per_label and llm_count >= target_per_label:\n",
    "            break\n",
    "    \n",
    "    # Combine and save\n",
    "    print(\"Combining and saving balanced dataset...\")\n",
    "    all_human = pd.concat(human_sampled, ignore_index=True)\n",
    "    all_llm = pd.concat(llm_sampled, ignore_index=True)\n",
    "    balanced_df = pd.concat([all_human, all_llm], ignore_index=True)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    balanced_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset created:\")\n",
    "    print(f\"Total size: {len(balanced_df):,}\")\n",
    "    print(balanced_df['label'].value_counts())\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# After running the batch processing, you can analyze results and create balanced dataset:\n",
    "# results = analyze_processed_results(\"processed_filtered_chunked_batch.csv\")\n",
    "# create_balanced_dataset_batch(\"processed_filtered_chunked_batch.csv\", \"balanced_processed.csv\", target_ratio=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "munin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
